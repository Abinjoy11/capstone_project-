# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tQG_qTepi46wEan_MJRBxb2kpseybk8y
"""

#This project is about the "steel industry electricity consumption data"
#project motive is to predicting the electricity usage in diffrent cases
#Part(A): EDA
#1.Preprocessing steps
#2.Identifying missing values
#3.Summary statistics
#4.Visualisation of relation of various factor on regarding the usage
#5.Correlation analysis
#6.Finding outliers and its visualisations

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

datas=pd.read_csv('Steel_industry_data.csv')

datas

datas.head()

datas.info()

datas.shape

datas= pd.read_csv("Steel_industry_data.csv")

# Step 1
datas['date'] = pd.to_datetime(datas['date'], format='%d/%m/%Y %H:%M', errors='coerce')

#  Step 2: Check and handle missing values
# Display missing values per column
missing_counts = datas.isnull().sum()
print("Missing values:\n", missing_counts)

# Optionally drop rows with missing critical values (e.g., date, Usage_kWh)
datas.dropna(subset=['date', 'Usage_kWh'], inplace=True)

#  Step 3: Remove duplicates
datas.drop_duplicates(inplace=True)

#  Step 4: Fix column names (strip whitespace and replace periods)
datas.columns = datas.columns.str.strip().str.replace('.', '_').str.replace('(', '').str.replace(')', '')

# Step 5: Convert data types (if needed)
numeric_columns = ['Usage_kWh', 'Lagging_Current_Reactive_Power_kVarh',
                   'Leading_Current_Reactive_Power_kVarh', 'CO2_(tCO2)',
                   'Lagging_Current_Power_Factor', 'Leading_Current_Power_Factor', 'NSM']


# Step 6: Fill or handle remaining missing values
# Here, we can choose to fill NA with 0 or use other imputation strategies
datas.fillna(0, inplace=True)

#  Final check
print("\nCleaned Data Overview:")
print(datas.info())

datas.describe()

#Bar plot for analysing the kWh usage in difference loads
sns.barplot(data=datas,x="Load_Type",y="Usage_kWh",color="r")

#Bar plot for analysing the kWh usage in different weeks
sns.barplot(data=datas,x="WeekStatus",y="Usage_kWh",color="r")

#KWH Usage analysis over days
sns.lineplot(data=datas,x="Day_of_week",y="Usage_kWh",marker="o")

#Scatter plot for showing the relation between Electricity usage and CO2 emission
datas = pd.read_csv("Steel_industry_data.csv")

# Filter out rows where CO2 emissions are zero
df_nonzero_co2 = datas[datas["CO2(tCO2)"] > 0]

# Calculate correlation between electricity usage and CO2 emissions
correlation = df_nonzero_co2["Usage_kWh"].corr(df_nonzero_co2["CO2(tCO2)"])

# Set plot style
sns.set(style="whitegrid")

# Create the scatter plot
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_nonzero_co2, x="Usage_kWh", y="CO2(tCO2)", alpha=0.6)
plt.title(f"Electricity Usage vs CO₂ Emissions (Non-zero CO₂ values)\nCorrelation = {correlation:.2f}")
plt.xlabel("Electricity Usage (kWh)")
plt.ylabel("CO₂ Emissions (tCO₂)")
plt.tight_layout()
plt.show()

#correlation heatmap for the numerical columns
# Compute the correlation matrix for numerical features
correlation_matrix = datas.corr(numeric_only=True)

# Set up the matplotlib figure
plt.figure(figsize=(10, 8))
sns.set(style="white")

# Draw the heatmap
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)

# Set plot title
plt.title("Correlation Heatmap of Steel Industry Features")
plt.tight_layout()

# Show the plot
plt.show()

#Detecting Outliers
plt.figure(figsize=(8,5))
sns.boxplot(datas['Usage_kWh'])
plt.title('Outliers in Total Usage kWh')
plt.xlabel('Total Usage')
plt.show()

#IQR Method for outlier detection
Q1=datas['Usage_kWh'].quantile(0.25)
Q3=datas['Usage_kWh'].quantile(0.75)

IQR= Q3-Q1
lower_bound=Q1-1.5*IQR
upper_bound=Q3+1.5*IQR
outliers=datas[(datas['Usage_kWh']<lower_bound) | (datas['Usage_kWh'] > upper_bound)]
print(f'number of outliers detected: {len(outliers)}')
print(outliers[['date','Usage_kWh']])

#Visaulisation of IQR outliers
plt.figure(figsize=(11,8))
sns.scatterplot(data=datas,x='date',y='Usage_kWh',label='Usage',color='blue')
sns.scatterplot(data=outliers,x='date',y='Usage_kWh',label='outliers',color='orange')
plt.title('Electricity usage with IQR outlires Highlited')
plt.xlabel('Date')
plt.ylabel('total usage')
plt.show()

#part b 'model building'

datas.head(3)

datas.dtypes

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression,Ridge,Lasso
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score

import pandas as pd

# Load the dataset
df = pd.read_csv("Steel_industry_data.csv")


# Drop the original 'date' column
df.drop(columns='date', inplace=True)

# Encode categorical variables
df_encoded = pd.get_dummies(df, columns=['WeekStatus', 'Day_of_week', 'Load_Type'], drop_first=True)

# Define features and target
X = df_encoded.drop(columns='Usage_kWh')
y = df_encoded['Usage_kWh']

# Display first 2 rows of features and target
print("X.head(2):\n", X.head(2))
print("\ny.head(2):\n", y.head(2))

#Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Train model
regressor = LinearRegression()
regressor.fit(X_train, y_train)

#Predictions
y_pred = regressor.predict(X_test)

y_pred=regressor.predict(X_test)
y_pred

y_test

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# Evaluation Metrics
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Absolute Error (MAE):", mae)
print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R² Score:", r2)

#Random forest regressor

from sklearn.ensemble import RandomForestRegressor

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)

# Define and train the model
rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)
rf_regressor.fit(X_train, y_train)

#Predictions
rf_preds = rf_regressor.predict(X_test)

#MAE and RMSE
import numpy as np
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_preds))
rf_mae=mean_absolute_error(y_test,rf_preds)
print('Mean Absolute Error (MAE):',round(rf_mae,2))
print('Root Mean Squared Error (RMSE):',round(rf_rmse,2))

#R2_Score
rf_r2=r2_score(y_test,rf_preds)
print("R2 Score(RandomForestRegressor):",round(rf_r2*100))

#Decision_Tree_Rregressor

from sklearn.tree import DecisionTreeRegressor

#Train the model
dt_model=DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train,y_train)

#Predict on the test data
dt_preds=dt_model.predict(X_test)

#MAE and RMSE
import numpy as np
dt_rmse = np.sqrt(mean_squared_error(y_test, dt_preds))

dt_mae=mean_absolute_error(y_test,dt_preds)
print('Mean Absolute Error (MAE):',round(dt_mae,2))
print('Root Mean Squared Error (RMSE):',round(dt_rmse,2))


#R2 Score
dt_r2=r2_score(y_test,dt_preds)
print("R2 Score (DecisionTreeRegressor):",round(dt_r2*100))

#KNN Regressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler

#scale features
scaler=StandardScaler()
X_train_scaled=scaler.fit_transform(X_train)
X_test_scaled=scaler.transform(X_test)

#Train KNN Regressor
knn_model=KNeighborsRegressor()
knn_model.fit(X_train_scaled,y_train)

#Predict
knn_preds=knn_model.predict(X_test_scaled)
#MAE and RMSE
import numpy as np
knn_rmse = np.sqrt(mean_squared_error(y_test, knn_preds))

knn_mae=mean_absolute_error(y_test,knn_preds)
print("Mean Absolute Error(MAE):",round(knn_mae,2))
print("Root Mean Squared Error(RMSE):",round(knn_rmse,2))

#R2 Score
knn_r2=r2_score(y_test,knn_preds)
print("R2 Score(KNeighborsRegressor):",round(knn_r2*100))

#SVR
from sklearn.svm import SVR

#Train Support vector Regressor
svr_model=SVR()
svr_model.fit(X_train_scaled,y_train)

#predict
svr_preds=svr_model.predict(X_test_scaled)
#MAE and RMSE
svr_rmse = np.sqrt(mean_squared_error(y_test, svr_preds))
svr_mae=mean_absolute_error(y_test,svr_preds)

print("Mean Absolute Error (MAE):",round(svr_mae,2))
print("Root Mean Squared Error(RMSE):",round(svr_rmse,2))

#R2 Score
svr_r2=r2_score(y_test,svr_preds)
print("R2 Score (SVR):",round(svr_r2*100))

#Ridge Regression
from sklearn.linear_model import Ridge

#Train_Ridge_Regression_Model
ridge_model=Ridge()
ridge_model.fit(X_train,y_train)

#Predict
ridge_preds=ridge_model.predict(X_test)

#MAE and RMSE
ridge_rmse=mean_squared_error(y_test,ridge_preds)
ridge_mae=mean_absolute_error(y_test,ridge_preds)

print("Mean Absolute Error(MAE):",round(ridge_mae,2))
print("Root Mean Squared Error(RMSE):",round(ridge_rmse,2))

#R2_Score
ridge_r2=r2_score(y_test,ridge_preds)
print("R2 Score(Ridge Regression):",round(ridge_r2*100))

#Lasso_Regressor
from sklearn.linear_model import Lasso

#Train Lasso Regression Model
lasso_model=Lasso()
lasso_model.fit(X_train,y_train)

#Predict
lasso_preds=lasso_model.predict(X_test)

#MAE and RMSE
lasso_rmse=mean_squared_error(y_test,lasso_preds)
lasso_mae=mean_absolute_error(y_test,lasso_preds)

print("Mean ABsolute Error(MAE):",round(lasso_mae,2))
print("Root Mean Squared Error(RMSE):",round(lasso_rmse,2))

#R2 Score
lasso_r2=r2_score(y_test,lasso_preds)
print("R2 Score (Lasso Regression):",round(lasso_r2*100))



# Collect R2 Scores from all models
r2_scores = {
    "Linear Regression": r2,
    "Random Forest Regressor": rf_r2,
    "Decision Tree Regressor": dt_r2,
    "KNN Regressor": knn_r2,
    "SVR": svr_r2,
    "Ridge Regression": ridge_r2,
    "Lasso Regression": lasso_r2
}

# Collect RMSE Values
RMSE_values = {
    "Linear Regression": rmse,
    "Random Forest Regressor": rf_rmse,
    "Decision Tree Regressor": dt_rmse,
    "KNN Regressor": knn_rmse,
    "SVR": svr_rmse,
    "Ridge Regression": ridge_rmse,
    "Lasso Regression": lasso_rmse
}

# Collect MAE Values
MAE_values = {
    "Linear Regression": mae,
    "Random Forest Regressor": rf_mae,
    "Decision Tree Regressor": dt_mae,
    "KNN Regressor": knn_mae,
    "SVR": svr_mae,
    "Ridge Regression": ridge_mae,
    "Lasso Regression": lasso_mae
}

# Create DataFrames for better visualization
r2_df = pd.DataFrame(list(r2_scores.items()), columns=["Model", "R2 Score"]).sort_values(by="R2 Score", ascending=False)
rmse_df = pd.DataFrame(list(RMSE_values.items()), columns=["Model", "RMSE"]).sort_values(by="RMSE")
mae_df = pd.DataFrame(list(MAE_values.items()), columns=["Model", "MAE"]).sort_values(by="MAE")

# Print Results
print("R2 Score Comparison:")
print(r2_df)

print("\nRMSE Comparison:")
print(rmse_df)

print("\nMAE Comparison:")
print(mae_df)

#Results from Regressions
#Random Forest Regressor performs the best, capturing complex patterns with high accuracy.

#Combine all evaluation metrics into a single DataFrame
# Combine all evaluation metrics into a single DataFrame
metrics_kwh = r2_df.merge(rmse_df, on="Model").merge(mae_df, on="Model")
print("\nModel Evaluation Summary: Steel Industry Dataset")
print(metrics_kwh)

import matplotlib.pyplot as plt
import seaborn as sns

import matplotlib.pyplot as plt
import seaborn as sns

# Set the plot style
sns.set(style="whitegrid")
plt.figure(figsize=(18, 5))

# Use the correct variable (metrics_kwh from your merged DataFrame)
# R² Score Plot
plt.subplot(1, 3, 1)
sns.barplot(data=metrics_kwh, x="R2 Score", y="Model", palette="viridis")
plt.title("R² Score (Higher is Better) - Steel Industry")
plt.xlim(0, 1)

# RMSE Plot
plt.subplot(1, 3, 2)
sns.barplot(data=metrics_kwh, x="RMSE", y="Model", palette="magma")
plt.title("RMSE (Lower is Better) - Steel Industry")

# MAE Plot
plt.subplot(1, 3, 3)
sns.barplot(data=metrics_kwh, x="MAE", y="Model", palette="coolwarm")
plt.title("MAE (Lower is Better) - Steel Industry")

plt.tight_layout()
plt.show()

datas.to_csv("steel_industry_cleaned.csv", index=False)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# 1. Load and preprocess
df = pd.read_csv("steel_industry_cleaned.csv")

# Parse & drop the timestamp
df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y %H:%M', errors='coerce')
df.drop(columns='date', inplace=True)

# 2. Define features and target
X = df.drop("Usage_kWh", axis=1)
y = df["Usage_kWh"]

# 3. One-hot encode categorical columns
X = pd.get_dummies(X, columns=["Load_Type", "WeekStatus", "Day_of_week"], drop_first=True)

# 4. Scale numeric features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# 6. Train the Random Forest Regressor
reg = RandomForestRegressor(n_estimators=100, random_state=42)
reg.fit(X_train, y_train)

# 7. Predict
y_pred = reg.predict(X_test)

# 8. Evaluate
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("Evaluation Metrics:")
print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"RMSE: {rmse}")
print(f"R² Score: {r2}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

# 1. Load data
df = pd.read_csv("steel_industry_cleaned.csv")

# 2. Parse & drop timestamp
df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y %H:%M', errors='coerce')
df.drop(columns='date', inplace=True)

# 3. Convert Usage_kWh to usage category
bins = [0, 30, 70, df["Usage_kWh"].max()]
labels = ['Low', 'Medium', 'High']
df['Usage_Category'] = pd.cut(df["Usage_kWh"], bins=bins, labels=labels)

# ❗ Drop rows with NaN in Usage_Category (these cause class 3 later)
df = df.dropna(subset=['Usage_Category'])

# 4. Define features and target
X = df.drop(["Usage_kWh", "Usage_Category"], axis=1)
y = df["Usage_Category"]

# 5. Encode target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# ✅ Check distribution (should have only 3 classes now)
print("Class distribution:", pd.Series(y_encoded).value_counts())

# 6. One-hot encode categorical features
X = pd.get_dummies(X, columns=["Load_Type", "WeekStatus", "Day_of_week"], drop_first=True)

# 7. Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 8. Train-test split (safe to stratify now)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# 9. Train classifier
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# 10. Evaluate
y_pred = clf.predict(X_test)

print("Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))
print("Confusion Matrix:\n")
print(confusion_matrix(y_test, y_pred))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv("steel_industry_cleaned.csv")

# 2. Convert 'date' to datetime and drop it
df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y %H:%M', errors='coerce')
df.drop(columns='date', inplace=True)

# 3. Bin the target variable into categories
bins = [0, 30, 70, df['Usage_kWh'].max()]
labels = ['Low', 'Medium', 'High']
df['Usage_Category'] = pd.cut(df['Usage_kWh'], bins=bins, labels=labels)

# ❗ Drop rows with NaN in Usage_Category (important!)
df = df.dropna(subset=['Usage_Category'])

# 4. Encode target variable
le = LabelEncoder()
y = le.fit_transform(df['Usage_Category'])  # 'Low' = 0, 'Medium' = 1, 'High' = 2

# 5. Prepare features
X = df.drop(columns=['Usage_kWh', 'Usage_Category'])

# 6. One-hot encode categorical features
X = pd.get_dummies(X, columns=['Load_Type', 'WeekStatus', 'Day_of_week'], drop_first=True)

# 7. Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 8. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 9. Train logistic regression model
model = LogisticRegression(max_iter=1000, multi_class='multinomial')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 10. Evaluation
print("Logistic Regression Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# 11. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# 12. Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Logistic Regression - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv("steel_industry_cleaned.csv")

# 2. Convert 'date' and drop it
df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y %H:%M', errors='coerce')
df.drop(columns='date', inplace=True)

# 3. Bin target variable
bins = [0, 30, 70, df['Usage_kWh'].max()]
labels = ['Low', 'Medium', 'High']
df['Usage_Category'] = pd.cut(df['Usage_kWh'], bins=bins, labels=labels)

# 4. Drop any rows with missing values in target
df = df.dropna(subset=['Usage_Category'])

# 5. Encode target labels
le = LabelEncoder()
y = le.fit_transform(df['Usage_Category'])  # 'Low'=0, 'Medium'=1, 'High'=2

# 6. Prepare features
X = df.drop(columns=['Usage_kWh', 'Usage_Category'])

# 7. One-hot encode categorical features
X = pd.get_dummies(X, columns=['Load_Type', 'WeekStatus', 'Day_of_week'], drop_first=True)

# 8. Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 9. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 10. Train KNN model
knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)

# 11. Classification report
print("KNN Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# 12. Confusion matrix
cm = confusion_matrix(y_test, y_pred)

# 13. Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("K-Nearest Neighbors - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv("steel_industry_cleaned.csv")

# 2. Convert 'date' and drop it
df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y %H:%M', errors='coerce')
df.drop(columns='date', inplace=True)

# 3. Bin target variable
bins = [0, 30, 70, df['Usage_kWh'].max()]
labels = ['Low', 'Medium', 'High']
df['Usage_Category'] = pd.cut(df['Usage_kWh'], bins=bins, labels=labels)

# 4. Drop rows with missing values in Usage_Category
df = df.dropna(subset=['Usage_Category'])

# 5. Encode target variable
le = LabelEncoder()
y = le.fit_transform(df['Usage_Category'])  # Low=0, Medium=1, High=2

# 6. Prepare features
X = df.drop(columns=['Usage_kWh', 'Usage_Category'])

# 7. One-hot encode categorical columns
X = pd.get_dummies(X, columns=['Load_Type', 'WeekStatus', 'Day_of_week'], drop_first=True)

# 8. Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 9. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 10. Train SVM model
model = SVC(kernel='linear')  # You can also try 'rbf', 'poly', etc.
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 11. Classification report
print("SVM Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# 12. Confusion matrix
cm = confusion_matrix(y_test, y_pred)

# 13. Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Support Vector Machine - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv("steel_industry_cleaned.csv")

# 2. Convert 'date' and drop it
df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y %H:%M', errors='coerce')
df.drop(columns='date', inplace=True)

# 3. Bin the target variable into classes
bins = [0, 30, 70, df['Usage_kWh'].max()]
labels = ['Low', 'Medium', 'High']
df['Usage_Category'] = pd.cut(df['Usage_kWh'], bins=bins, labels=labels)

# 4. Drop any rows with missing target values
df = df.dropna(subset=['Usage_Category'])

# 5. Encode target variable
le = LabelEncoder()
y = le.fit_transform(df['Usage_Category'])  # Low=0, Medium=1, High=2

# 6. Prepare features
X = df.drop(columns=['Usage_kWh', 'Usage_Category'])

# 7. One-hot encode categorical features
X = pd.get_dummies(X, columns=['Load_Type', 'WeekStatus', 'Day_of_week'], drop_first=True)

# 8. Feature scaling (optional for trees, included for consistency)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 9. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 10. Train Decision Tree model
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 11. Evaluation
print("Decision Tree Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# 12. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# 13. Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Decision Tree - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv("steel_industry_cleaned.csv")

# 2. Convert 'date' and drop it
df['date'] = pd.to_datetime(df['date'], format='%d/%m/%Y %H:%M', errors='coerce')
df.drop(columns='date', inplace=True)

# 3. Bin the target variable
bins = [0, 30, 70, df['Usage_kWh'].max()]
labels = ['Low', 'Medium', 'High']
df['Usage_Category'] = pd.cut(df['Usage_kWh'], bins=bins, labels=labels)

# 4. Drop rows with missing target
df = df.dropna(subset=['Usage_Category'])

# 5. Encode target
le = LabelEncoder()
y = le.fit_transform(df['Usage_Category'])  # Low=0, Medium=1, High=2

# 6. Prepare features
X = df.drop(columns=['Usage_kWh', 'Usage_Category'])

# 7. One-hot encode categorical features
X = pd.get_dummies(X, columns=['Load_Type', 'WeekStatus', 'Day_of_week'], drop_first=True)

# 8. Feature scaling (optional for trees, included for consistency)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 9. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42, stratify=y
)

# 10. Train Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 11. Evaluation
print("Random Forest Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# 12. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)

# 13. Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Random Forest - Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

#Collect accuracies score from all models

# Store models in a dictionary
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM": SVC(kernel='linear'),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

# Dictionary to hold accuracy scores
accuracy_scores = {}

# Loop through models and evaluate
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracy_scores[name] = acc

# Print all accuracy scores
print("\nAccuracy Scores from All Models:")
for model_name, acc in accuracy_scores.items():
    print(f"{model_name}: {acc:.4f}")

#The Decision tree classifier delivered the highest accuracy

import matplotlib.pyplot as plt

# Plotting
plt.figure(figsize=(10, 6))
plt.bar(ac_scores.keys(), ac_scores.values(), color='skyblue', edgecolor='black')

plt.title("Model Accuracy Comparison", fontsize=14)
plt.xlabel("Model", fontsize=12)
plt.ylabel("Accuracy Score", fontsize=12)
plt.ylim(0.6, 1.0)
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Add accuracy values above bars
for model, acc in ac_scores.items():
    plt.text(model, acc + 0.005, f"{acc:.3f}", ha='center', fontsize=10)

plt.tight_layout()
plt.show()

#Conclusion Decision Tree Regression emerged as the best model for both regression and classification tasks,